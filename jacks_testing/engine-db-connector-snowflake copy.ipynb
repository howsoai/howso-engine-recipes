{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89c7783a-b0f4-4e2d-9c7c-0663c41f86de",
   "metadata": {},
   "source": [
    "# Engine: Database Connection - Snowflake \n",
    "\n",
    "### Overview: \n",
    "\n",
    "Because Howso Engine is an API-first platform built upon Pandas DataFrames, it is very easy to connect to most databases. This includes Amazon AWS, Google GCP, Microsoft Azure Studios, and Snowflake. \n",
    "\n",
    "In this notebook, we will adapt the [1-engine-intro.ipynb](https://github.com/howsoai/howso-engine-recipes/blob/main/1-engine-intro.ipynb) recipe to read-in a dataset from Snowflake. For information on your specific data warehouse, please consult your data vendor's documentation.\n",
    "\n",
    "### Recipe Goals:\n",
    "\n",
    "This notebook will provide a demonstration of how to load a table from Snowflake. To follow along with the recipe, please download the [Adult dataset](https://github.com/howsoai/howso-engine-recipes/blob/main/data/adult/adult/adult.tsv.gz) and upload it to your Snowflake account. The exact details of how to do this are beyond the scope of this recipe, but there are numerous resources online.\n",
    "\n",
    "**Note**: This notebook assumes a user has a source dataset within Snowflake and have all the credentials needed access it. They should also be familiar with basic SQL statements.   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d573a6",
   "metadata": {},
   "source": [
    "Once the dataset is uploaded, install the [Snowflake Python/Pandas connector](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas#requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81421b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://jack.xia%40howso.com:****@dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/simple\n",
      "Collecting snowflake-connector-python[pandas]\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/15/27/a16d8c64d44126086bec10c35cd7915a6f2b0f4dd86f2d6552679aba34bb/snowflake_connector_python-3.7.1-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.9/949.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/c9/7f/09065fd9e27da0eda08b4d6897f1c13535066174cc023af248fc2a8d5e5a/asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.9 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (1.16.0)\n",
      "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (42.0.2)\n",
      "Collecting pyOpenSSL<25.0.0,>=16.2.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/54/a7/2104f674a5a6845b04c8ff01659becc6b8978ca410b82b94287e0b1e018b/pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyjwt<3.0.0 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (2.8.0)\n",
      "Requirement already satisfied: pytz in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (2.31.0)\n",
      "Requirement already satisfied: packaging in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (4.9.0)\n",
      "Collecting filelock<4,>=3.5 (from snowflake-connector-python[pandas])\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/8b/69/acdf492db27dea7be5c63053230130e0574fd8a376de3555d5f8bbc3d3ad/filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Collecting sortedcontainers>=2.4.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting platformdirs<4.0.0,>=2.6.0 (from snowflake-connector-python[pandas])\n",
      "  Downloading https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/56/29/3ec311dc18804409ecf0d2b09caa976f3ae6215559306b5b530004e11156/platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Collecting tomlkit (from snowflake-connector-python[pandas])\n",
      "  Using cached https://dpbuild.jfrog.io/artifactory/api/pypi/vpypi-edge/packages/packages/07/fa/c96545d741f2fd47f565e4e06bfef0962add790cb9c2289d900102b55eca/tomlkit-0.12.4-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (2.1.4)\n",
      "Requirement already satisfied: pyarrow in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from snowflake-connector-python[pandas]) (14.0.2)\n",
      "Requirement already satisfied: pycparser in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.21)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (2023.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jackxia/.pyenv/versions/3.11.2/envs/engine_rec_311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.0.0->snowflake-connector-python[pandas]) (1.16.0)\n",
      "Installing collected packages: sortedcontainers, asn1crypto, tomlkit, platformdirs, filelock, pyOpenSSL, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 filelock-3.13.3 platformdirs-3.11.0 pyOpenSSL-24.1.0 snowflake-connector-python-3.7.1 sortedcontainers-2.4.0 tomlkit-0.12.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Activate the correct environment and then:\n",
    "%pip install \"sqlalchemy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729f6c7a-4b11-4311-aad9-03f89a568e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pmlb import fetch_data\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from howso.engine import Trainee, load_trainee\n",
    "from howso.utilities import infer_feature_attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc81651d-62ab-4ac2-80d1-a58ddeaa0413",
   "metadata": {},
   "source": [
    "## Step 1: Import Data\n",
    "\n",
    "Connecting to a database is akin to scanning a badge to access your office. It requires all the correct credentials. For security reasons, your sensitive information is typically not supposed to be exposed in the code. In this example, the private information is masked and it is implied you must input your own credentials to access your dataset. Please consult your database or IT administrator for your organization's procedures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3954602d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOUR_user_information' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnowflake\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# establish connection\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ctx \u001b[38;5;241m=\u001b[39m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m----> 5\u001b[0m     user\u001b[38;5;241m=\u001b[39m\u001b[43mYOUR_user_information\u001b[49m,\n\u001b[1;32m      6\u001b[0m     password\u001b[38;5;241m=\u001b[39mYOUR_password_information,\n\u001b[1;32m      7\u001b[0m     account\u001b[38;5;241m=\u001b[39mYOUR_account_information,\n\u001b[1;32m      8\u001b[0m     warehouse\u001b[38;5;241m=\u001b[39mYOUR_warehouse,\n\u001b[1;32m      9\u001b[0m     database\u001b[38;5;241m=\u001b[39mYOUR_database,\n\u001b[1;32m     10\u001b[0m     schema\u001b[38;5;241m=\u001b[39mYOUR_schema,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m cs \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# retrieve the table (dataset)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOUR_user_information' is not defined"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# establish connection\n",
    "ctx = snowflake.connector.connect(\n",
    "    user=YOUR_user_information,\n",
    "    password=YOUR_password_information,\n",
    "    account=YOUR_account_information,\n",
    "    warehouse=YOUR_warehouse,\n",
    "    database=YOUR_database,\n",
    "    schema=YOUR_schema,\n",
    ")\n",
    "cs = ctx.cursor()\n",
    "\n",
    "# retrieve the table (dataset)\n",
    "source_table_name = \"adult\"\n",
    "\n",
    "try:\n",
    "    sql = f'''\n",
    "        SELECT *\n",
    "        FROM \"{source_table_name}\"\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 2000\n",
    "        '''\n",
    "    cs.execute(sql)\n",
    "    df = cs.fetch_pandas_all()\n",
    "    display(Markdown(f'### \"{source_table_name.capitalize()}\" table:'))\n",
    "    display(df)\n",
    "\n",
    "# close connection to database\n",
    "finally:\n",
    "    cs.close()\n",
    "ctx.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4910b04",
   "metadata": {},
   "source": [
    "Our example dataset for this recipe is the well known `Adult` dataset. This dataset consists of 14 Context Features and 1 Action Feature. The Action Feature in this version of the `Adult` dataset has been renamed to `target` and it takes the form of a binary indicator for whether a person in the data makes more than $50,000/year (*target*=1) or less (*target*=0)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec71ebee-2879-418a-a3d4-94da3e5cecd1",
   "metadata": {},
   "source": [
    "## Step 2: Feature Mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27ff9e8c-abfa-4408-a631-2ce01edf05b5",
   "metadata": {},
   "source": [
    "Typically, an exploratory analysis is done on the data to get a general feel of the descriptive statistics and data attributes. \n",
    "\n",
    "Methods like `describe` from a Pandas dataframe often automatically present these types of information of interest to a user, as shown below. While informative, these descriptive statistics are often used as a sanity check pre- and post-modeling and a model typically doesn't actually use any of these feature attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12242480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.615000</td>\n",
       "      <td>3.862000</td>\n",
       "      <td>189210.028500</td>\n",
       "      <td>10.273000</td>\n",
       "      <td>10.158000</td>\n",
       "      <td>2.636000</td>\n",
       "      <td>6.466500</td>\n",
       "      <td>1.467500</td>\n",
       "      <td>3.652000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>1314.236000</td>\n",
       "      <td>97.251500</td>\n",
       "      <td>40.34200</td>\n",
       "      <td>36.782000</td>\n",
       "      <td>0.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.801812</td>\n",
       "      <td>1.478529</td>\n",
       "      <td>106386.888184</td>\n",
       "      <td>3.721881</td>\n",
       "      <td>2.597578</td>\n",
       "      <td>1.516461</td>\n",
       "      <td>4.218504</td>\n",
       "      <td>1.608178</td>\n",
       "      <td>0.856308</td>\n",
       "      <td>0.469602</td>\n",
       "      <td>8699.654387</td>\n",
       "      <td>429.418206</td>\n",
       "      <td>12.15092</td>\n",
       "      <td>7.756582</td>\n",
       "      <td>0.429302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19302.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>116983.250000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>174946.500000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>239621.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>860348.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>3175.000000</td>\n",
       "      <td>99.00000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age    workclass         fnlwgt    education  education-num  \\\n",
       "count  2000.000000  2000.000000    2000.000000  2000.000000    2000.000000   \n",
       "mean     38.615000     3.862000  189210.028500    10.273000      10.158000   \n",
       "std      13.801812     1.478529  106386.888184     3.721881       2.597578   \n",
       "min      17.000000     0.000000   19302.000000     0.000000       1.000000   \n",
       "25%      28.000000     4.000000  116983.250000     9.000000       9.000000   \n",
       "50%      37.000000     4.000000  174946.500000    11.000000      10.000000   \n",
       "75%      48.000000     4.000000  239621.000000    12.000000      13.000000   \n",
       "max      90.000000     8.000000  860348.000000    15.000000      16.000000   \n",
       "\n",
       "       marital-status   occupation  relationship         race          sex  \\\n",
       "count     2000.000000  2000.000000   2000.000000  2000.000000  2000.000000   \n",
       "mean         2.636000     6.466500      1.467500     3.652000     0.672000   \n",
       "std          1.516461     4.218504      1.608178     0.856308     0.469602   \n",
       "min          0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%          2.000000     3.000000      0.000000     4.000000     0.000000   \n",
       "50%          2.000000     6.000000      1.000000     4.000000     1.000000   \n",
       "75%          4.000000    10.000000      3.000000     4.000000     1.000000   \n",
       "max          6.000000    14.000000      5.000000     4.000000     1.000000   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country       target  \n",
       "count   2000.000000   2000.000000      2000.00000     2000.000000  2000.000000  \n",
       "mean    1314.236000     97.251500        40.34200       36.782000     0.756500  \n",
       "std     8699.654387    429.418206        12.15092        7.756582     0.429302  \n",
       "min        0.000000      0.000000         2.00000        0.000000     0.000000  \n",
       "25%        0.000000      0.000000        40.00000       39.000000     1.000000  \n",
       "50%        0.000000      0.000000        40.00000       39.000000     1.000000  \n",
       "75%        0.000000      0.000000        45.00000       39.000000     1.000000  \n",
       "max    99999.000000   3175.000000        99.00000       41.000000     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d1b856f",
   "metadata": {},
   "source": [
    "In the Howso Engine workflow, feature attributes are an essential part of model building and usage. By incorporating certain feature attributes into training process itself, Howso Engine gains another layer of information that will help in fine-tuning the results. \n",
    "\n",
    "In order to assist the user with defining the feature attributes, Howso has an `infer_feature_attributes` tool that automatically processes the dataset for the user.\n",
    "\n",
    "In Howso Engine, these feature attributes are model infrastructure feature parameters that are based on the existing data, rather than exact descriptive statistics. This is why, for example, the min and max bounds on continuous features are not the exact min and max values of the dataset, but rather an expanded version of those min and max values to allow for some variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102736cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 7.38905609893065, 'max': 148.4131591025766}},\n",
       " 'workclass': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'fnlwgt': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 8103.083927575384, 'max': 1202604.2841647768}},\n",
       " 'education': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'education-num': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 1.0, 'max': 20.085536923187668}},\n",
       " 'marital-status': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'occupation': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'relationship': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'race': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'sex': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'capital-gain': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 0.0, 'max': 162754.79141900392}},\n",
       " 'capital-loss': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 0.0, 'max': 8103.083927575384}},\n",
       " 'hours-per-week': {'type': 'continuous',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'numeric', 'size': 8},\n",
       "  'bounds': {'min': 1.0, 'max': 148.4131591025766}},\n",
       " 'native-country': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}},\n",
       " 'target': {'type': 'nominal',\n",
       "  'data_type': 'number',\n",
       "  'decimal_places': 0,\n",
       "  'original_type': {'data_type': 'integer', 'size': 1},\n",
       "  'bounds': {'allow_null': False}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infer features attributes\n",
    "features = infer_feature_attributes(df)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5999f86-1baf-4a67-b849-c9a8f73e2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Context and Action Features\n",
    "action_features = ['target']\n",
    "context_features = features.get_names(without=action_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4fe66d7",
   "metadata": {},
   "source": [
    "> **Note:** Train Test Split\n",
    "\n",
    "To gauge model performance, train-test splits are often used in traditional machine learning workflows. Howso Engine does not require the use of train-test split for validation. Please see recipe `6-validation.ipynb` for further explanation. Therefore, we will not use train-test splits our recipes unless the test set serves a specific purpose."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c047722c-3d5e-4c88-ba05-59cfe5f82325",
   "metadata": {},
   "source": [
    "## Step 3: Create Trainee"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce2a88f1-6ca1-4c75-a545-760710946e35",
   "metadata": {},
   "source": [
    "To begin the Howso Engine workflow, a Trainee is created to act as a base for all of our ML needs. In all subsequent notebooks and jupyter cells, we will refer to Howso Engine's model as Trainee.\n",
    "\n",
    "**`Definitions`:**\n",
    "\n",
    "**`Trainee`:** A collection of Cases that comprise knowledge. May include metadata and parameters. In traditional ML this is referred to as a model.\n",
    "\n",
    "**`Cases`:** A set of feature values representing a situation observed.  In traditional ML, a Case is sometimes referred to as an \"observation\", \"record\", or \"data point\". In database terms, a Case would be a row of values. For supervised learning a Case is a set of Context Features and Action Features and for unsupervised learning a Case is just a set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71444a54-555e-4147-9535-ceff2e42440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 9.1.2 of Howso Engine™ is available. You are using version 7.1.0.\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainee\n",
    "t = Trainee(\n",
    "    features=features,  \n",
    "    overwrite_existing=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd0bdc6",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing and Training\n",
    "\n",
    "One benefit of Howso Engine is that most standard forms of data pre-processing such as one hot encoding and standardizing are `NOT` needed, which is in contrast to many traditional ML models. This does not include more sophisticated forms of pre-processing such as feature selection or feature engineering, which may still be useful. Fitting is also done in two steps in Howso Engine.\n",
    "\n",
    "**`Definitions`:**\n",
    "\n",
    "**`Train`:** Exposing a Trainee to a Case which may cause the ML algorithm to update the Trainee. This is a single training step; training may happen at each decision, at a certain sampling rate of observations per second, or at certain events.\n",
    "\n",
    "**`Analyze`:** Tune internal parameters to improve performance and accuracy of predictions and metrics. Analysis may be targeted or targetless.  Targetless analysis provides the best balanced set of hyperparameters if an Action Feature is not specified, along with a performance boost while targeted analysis provides a boost to accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "t.train(df)\n",
    "\n",
    "# Analyze the Trainee\n",
    "t.analyze(context_features=context_features, action_features=action_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bceee584",
   "metadata": {},
   "source": [
    "## Step 5: Results\n",
    "\n",
    "Once Howso Engine is trained and analyzed, it provides the user with a variety of ML capabilities. At this stage in the Howso Engine workflow, a typical use case would be to evaluate the accuracy of the Trainee, which is performed by the `react` method. This is equivalent to `predict` in many traditional Machine Learning workflows, although the `react` method is not solely used for supervised predictions as detailed in subsequent recipes. \n",
    "\n",
    "Since we are not using a train-test split, we will use the `react_into_trainee` method, which performs a `react` on each of the cases that is trained into the model. Alternatively, a standard `react` call may be used on unseen data for prediction.\n",
    "The accuracy is calculated internally as shown in the code below and this is the recommended accuracy metric. Further explanations are available in recipe `6-validation`.\n",
    "\n",
    "**`Definitions`:**\n",
    "\n",
    "**`React`:** Exposing a Trainee to a new Case's Context Features and an Action Feature for that case is returned. In traditional ML this is often referred to predicting or labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14e534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howso Prediction Results - Accuracy: 0.83, Precision: 0.77, and Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Recommended metrics\n",
    "t.react_into_trainee(action_feature=action_features[0], residuals=True)\n",
    "stats = t.react_aggregate().target.round(2)\n",
    "print(f'Howso Prediction Results - Accuracy: {stats[\"accuracy\"]}, Precision: {stats[\"recall\"]}, and Recall: {stats[\"precision\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b772af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howso Prediction Results - Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual       0    1\n",
       "Predicted          \n",
       "0          137   69\n",
       "1           99  695"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats_matrix = t.react_aggregate(stats=[\"confusion_matrix\"])\n",
    "print(\"Howso Prediction Results - Confusion Matrix\")\n",
    "matrix = pd.DataFrame(stats_matrix.loc[\"confusion_matrix\", \"target\"])\n",
    "matrix.index.name = \"Predicted\"\n",
    "matrix.columns.name = \"Actual\"\n",
    "display(matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78d9072a",
   "metadata": {},
   "source": [
    "Another way to use the `react_aggregate` method is to utilize the `condition` parameter. Specifying a condition allows the user to get prediction stats on the subset of cases that matches the specified condition.\n",
    "\n",
    "For example, we can use the `condition` parameter to find the accuracy, precision, and recall on the subsets of the dataset with each value for Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f0a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howso Prediction Results On Cases with Sex=1 - Accuracy: 0.8, Precision: 0.77, and Recall: 0.75\n",
      "Howso Prediction Results On Cases with Sex=0 - Accuracy: 0.93, Precision: 0.85, and Recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "sex_1_stats = t.react_aggregate(condition={'sex': 1})['target'].round(2)\n",
    "print(f'Howso Prediction Results On Cases with Sex=1 - Accuracy: {sex_1_stats[\"accuracy\"]}, Precision: {sex_1_stats[\"recall\"]}, and Recall: {sex_1_stats[\"precision\"]}')\n",
    "\n",
    "sex_0_stats = t.react_aggregate(condition={'sex': 0})['target'].round(2)\n",
    "print(f'Howso Prediction Results On Cases with Sex=0 - Accuracy: {sex_0_stats[\"accuracy\"]}, Precision: {sex_0_stats[\"recall\"]}, and Recall: {sex_0_stats[\"precision\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c7c59c7",
   "metadata": {},
   "source": [
    "## Step 6: Saving, Loading, and Deleting from your local disk\n",
    "\n",
    "These methods work for `HowsoDirectClient` and its subclasses.\n",
    "\n",
    "### Saving and Loading\n",
    "\n",
    "When saving to disk, a filepath or filename may provided. The filepath provided may be a relative or absolute path. The filepath can include the desired filename with a `.caml` extension. If no filename is at the end of the filepath, the Trainee ID will be used. If just the filename is provided, then the current working directory will be used as the filepath. \n",
    "\n",
    "> **Note:** It is recommended, however not necessary that the filename match the trainee ID\n",
    "\n",
    "\n",
    "### Deleting\n",
    "\n",
    "When deleting a Trainee, the `delete` method deletes the trainee from the last saved or loaded disk location, as well as memory. If the Trainee has not been saved, `delete` can also be used to just remove a Trainee from memory.\n",
    "\n",
    "When Trainees are saved, a `.caml` file and a `.txt` version file is saved. The `delete` method assumes that the prefix to the version filename is the same as the `.caml` trainee filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from howso.direct import HowsoDirectClient\n",
    "\n",
    "# Clients that extend `HowsoDirectClient` will have local file operations\n",
    "if isinstance(t.client, HowsoDirectClient):\n",
    "    # Example filepath (same as default)\n",
    "    cwd = Path.cwd()\n",
    "    file_path=f\"{cwd}/engine_intro_trainee.caml\"\n",
    "\n",
    "    # Saving\n",
    "    t.save(file_path=file_path) \n",
    "\n",
    "    # Loading\n",
    "    t = load_trainee(file_path=file_path)\n",
    "\n",
    "    # Deleting\n",
    "    t.delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5023a91b",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Databases are ubiquititous in the era of \"Big Data,\" and it is imperative solutions can connect to an organization's data warehouse. By being an API-first platform, Howso offers the freedom and flexibility to connect to many of the popular cloud warehouses. \n",
    "\n",
    "Use this recipe as a template for connecting to your data, and ultimately, building your use-cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9b4298285f63347a4f5469a5c77e773aa0d93492a1da372f14781d93b4190a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
