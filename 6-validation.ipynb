{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bed22ad",
   "metadata": {},
   "source": [
    "# Recipe 6: Instance Based Learning\n",
    "## Overview \n",
    "\n",
    "Recipes 1-5 provided an overview of the many Howso Engine capabilties. In many of these recipes, we refer to Howso Engine's system as instance-based learning. Many of these methods used are only available in Howso Engine due to the unique combination of instance based learning combined with information theory. This notebook will provide a deeper demonstration of one of Howso Engine's functionalities that arises from instance-based learning.\n",
    "\n",
    "Since most modern methods use model-based learning, many of the standard practices in machine learning revolve around model-based approaches. There are also many commonly accepted machine learning paradigms that also arise from this model-based learning approach. One of the most commonly used practices is a train-test split for model validation. Since Howso Engine performs an accuracy validation calculation that surpasses the scope of many train-test splits, we recommend using Howso Engine's Feature Residuals instead of using a train-test split. As many of our methods and recommendations, including this, may seem counterintuitive compared to standard machine learning practices, it is important for us to provide empirical validation.\n",
    "\n",
    "Thus, this Recipe is designed to provide context and proof for Howso's recommendation that train-test splits are uncessary for Trainee evaluation through the use of our `Feature Residuals`. This recommendation is to save the user time, remove complexity, and alleviate possible data constraints. Users are always welcome to continue using train-test validation splits but it our hope is to provide ample evidence that a train-test split does not provide any benefit when using Howso Engine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2e3ffff",
   "metadata": {},
   "source": [
    "## Recipe Goals: \n",
    "This recipe conducts an experiment to provide evidence Howso Engine's recommendation to use Feature Residuals instead of train-test splits. This is done not only to justify the recommendation, but also provide a further details on instance-based learning by showing an example of how Howso Engine leverages instance-based learning's unique attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8c989b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T15:45:29.074017Z",
     "iopub.status.busy": "2024-02-09T15:45:29.073848Z",
     "iopub.status.idle": "2024-02-09T15:45:29.831825Z",
     "shell.execute_reply": "2024-02-09T15:45:29.831321Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmlb import fetch_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from howso import engine\n",
    "from howso.utilities import infer_feature_attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71b2a48d",
   "metadata": {},
   "source": [
    "# Instance-Based Learning\n",
    "\n",
    "Instance based learning is a type of learning algorithms that compares new instances (or data) with instances seen during training, which have been stored in memory. One of our favorite phrases is \"the model is the data\" when refering to Howso Engine's used of instance-based learning. Ultimately, instance-based learning allows Howso Engine to provide a system that is truly representative of the data, instead of a generalization in the form of a model. \n",
    "\n",
    "In modern machine learning, model-based learning methods dominate the industry. This is due to model-based learning methods generally having better generalization and performance. Traditionally, some model-based methods present better explainability, for example the coefficients of regression models provide information on each feature. As machine learning methods advance, the traditional easy to compute, closed-form solutions have often given way to powerful and complex algorithms such as Neural Networks. These methods can offer impressive performance, but often at an even bigger tradeoff for interpretability. \n",
    "\n",
    "Due to Howso Engine's advanced querying system and novel use of information theory, Howso Engine has overcome many of the challenges associated with instance-based learning, allowing it to take advantage of its existing advantages without sacrificing performance. This recipe is not designed to go deep into the underlying math. For more information, please refer to Howso's whitepaper <em>Natively Interpretable Machine Learning and Artificial Intelligence: Preliminary Results and Future Directions. </em> (Hazard et. al. 2019)\n",
    "`https://arxiv.org/abs/1901.00246`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9442de9c",
   "metadata": {},
   "source": [
    "# Residuals\n",
    "\n",
    "Why are instance-based models so flexible? Because the data is memorized or saved in the Trainee, dynamic editing and calculation is enabled. \n",
    "\n",
    "> Note: Residuals and Error are often used interchangably. Even within, Howso Engine, there are multiple forms of Residuals. To clarify, Feature Residuals can be considered the exact same metric as the Mean Absolute Error (MAE). We will use the term Feature Residuals when refering to Howso Engine calculations and Error or MAE when refering to calculations from non-Howso Engine models. However, from a mathematics standpoint, Feature Residuals, Error, and MAE are the equivalent, which will be demonstrated.\n",
    "\n",
    "In traditional machine learning workflows, a train-test split is used to validate the model and measure its performance. If a test dataset is not used, testing on the training data will often over-inflate the validation results because the model has seen the data before. Train-test splits are also often done in large chunks, with a 20% test set being a common heuristic. Unless a user uses enough train-test split iterations to ensure that every data point has been tested, there is also the danger of undercoverage in terms of validation. The difficulty with taking more chunks is that a model needs to be retrained every time, thus every model is different. When all of the test set results are aggregated, how do you aggregate the models? This is often impossible to do and usually the results are an indicator of how well the underlying method, e.g., regression, decision tree, Neural Network etc., and its corresponding set of hyperparameters is at creating a model for the data, but not necessarily how good a specific model is.\n",
    "\n",
    "How does Howso Engine provide a solution to these issues? Instance-based learning allows Howso to hold out every Case one at a time and predict a feature for that Case. After the prediction, the Case is return and instantly becomes part of the Trainee again. By repeating and doing this for every case, Howso Engine provides a comprehensive `Feature Residual` calculation that emcompasses every single Case. Since the held out Case becomes part of the Trainee again, there is no need to retrain the Trainee. The `Feature Residual` represents one of the most comprehensive forms of validation, leave-one-out, that is often times prohibitively expensive in other models. The results are also directly reflective of the performance of the current Trainee, and thus model, so it can be immediately used with exact knowledge of its current state of performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f139541",
   "metadata": {},
   "source": [
    "# Section 1. Setup\n",
    "\n",
    "We will continue to use the `Adult` dataset. In previous receipes, we explicitly used `target` as our Action Feature, however for this experiment, we will iterate through all of the features and use one feature every iteration until every feature has been used as an Action Feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31730cc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T15:45:29.834848Z",
     "iopub.status.busy": "2024-02-09T15:45:29.834421Z",
     "iopub.status.idle": "2024-02-09T15:45:29.899644Z",
     "shell.execute_reply": "2024-02-09T15:45:29.899089Z"
    }
   },
   "outputs": [],
   "source": [
    "df = fetch_data('adult', local_cache_dir=\"data/adult\")\n",
    "\n",
    "# Subsample the data to ensure the example runs quickly\n",
    "df = df.sample(500)\n",
    "\n",
    "features = infer_feature_attributes(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a96d52a1",
   "metadata": {},
   "source": [
    "# Section 2. Experiment\n",
    "\n",
    "We will demonstrate that Howso Engine's Global Feature Residuals is equivalent to calculating the prediction MAE.  \n",
    "\n",
    "For every iteration:  \n",
    "\n",
    "> 1. We will create, train, and analyze a Trainee\n",
    "> 2. We will then extract the Global Feature Residuals. Under Howso's recommended workflow, this would be the only metric needed for validation\n",
    "\n",
    "To test, within every iteration, for every feature:\n",
    "\n",
    "> 3. We split the test set and use that feature as the Action Feature and every other feature as Context Features\n",
    "> 4. We compute the MAE either through the prediction results or the `categorical_action_probabilities` for nominal Action Features\n",
    "\n",
    "Finally:    \n",
    "> 5. We compare the Global Feature Residuals with the MAE for every feature\n",
    "\n",
    "In the following code, we will use more in-line comments as we cannot break up this section of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c27e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T15:45:29.902458Z",
     "iopub.status.busy": "2024-02-09T15:45:29.902264Z",
     "iopub.status.idle": "2024-02-09T15:53:39.813569Z",
     "shell.execute_reply": "2024-02-09T15:53:39.813029Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Run:  9\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "result_columns = ['Global Feature Residuals', 'Prediction MAE']\n",
    "\n",
    "# Create the results holder\n",
    "all_df_results = dict()\n",
    "for feature in list(df):\n",
    "    all_df_results[feature] = pd.DataFrame(columns=result_columns) \n",
    "\n",
    "# Main experiment loop   \n",
    "for run in range(0,9):\n",
    "    print(\"Begin Run: \", run+1)\n",
    "    \n",
    "    \"\"\"\n",
    "    We start by using a train-test split. The goal of this recipe is to demonstrate\n",
    "    that this step is no longer needed, however our experiment will use it for \n",
    "    comparison.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2) #,random_state=42)\n",
    "\n",
    "    # Create the Trainee\n",
    "    trainee = engine.Trainee(features=features)\n",
    "\n",
    "    # Analyze and Train the Trainee on the test dataset\n",
    "    trainee.train(train_df)\n",
    "    trainee.analyze()\n",
    "    \n",
    "    \"\"\"\n",
    "    These Residuals are the full residuals that the Trainee automatically calculates when using\n",
    "    `reacting_into_trainee` and the Residual that Howso recommends as a replacement for the\n",
    "    manual Mean Absolute Error calculations from a train-test split.\n",
    "    \"\"\"\n",
    "    # Compute and return full residuals\n",
    "    trainee.react_into_trainee(residuals=True)\n",
    "    residuals = trainee.get_prediction_stats(robust=False, stats=['mae'])\n",
    "    \n",
    "    \"\"\"\n",
    "    To provide a robust comparison, we loop through every feature. In each loop, one feature is\n",
    "    selected as the Action Feature and the remaining features are the Context Features that predict\n",
    "    this Action Feature, exactly like the standard workflow for predicting a target feature.\n",
    "    \"\"\"\n",
    "    for feature in list(df):\n",
    "\n",
    "        action_features = [feature]\n",
    "\n",
    "        # Split the data into context features (X) and action feature (y)\n",
    "        X_test = test_df.drop(feature, axis=1)\n",
    "        y_test = test_df[action_features[0]]\n",
    "\n",
    "        context_features = X_test.columns.tolist()  \n",
    "\n",
    "        # Designate whether the feature is continous or nominal for metric selection \n",
    "        feature_is_nominal = features[feature]['type'] == 'nominal'\n",
    "\n",
    "        residual = residuals[feature].iloc[0]\n",
    "        \"\"\"\n",
    "        Since we are also predicting nominal features, categorical_action_probabilities are used to\n",
    "        calculate the MAE instead of exact predictions as it provides a more accurate representation\n",
    "        of the model prediction. A more detailed explanation is available later in this section.\n",
    "        \"\"\"\n",
    "        details={'categorical_action_probabilities': feature_is_nominal, 'feature_residuals': True}\n",
    "\n",
    "        results = trainee.react(\n",
    "            X_test, \n",
    "            context_features=context_features, \n",
    "            action_features=action_features,\n",
    "            details=details\n",
    "        )\n",
    "        \n",
    "        # Determine metric\n",
    "        if feature_is_nominal:\n",
    "            \"\"\"\n",
    "            For nominal features, we can use the `categorical_action_probabilities` to get a better \n",
    "            estimate of the MAE. `categorical_action_probabilities` tells use the probability the Trainee\n",
    "            assigns for each possible outcome. For example, if we are predicting a nominal feature where the\n",
    "            possible outcomes are the nominal integers of 1 and 2, then each one of those outcomes will be \n",
    "            associated with a probability of prediction. This means that if nominal 1 has a probability of 0.55,\n",
    "            then the Trainee believes that the case has a 55% chance of being a 1. Since the Trainee has to\n",
    "            predict a value for the case, the final predicted value is the possible value with the highest\n",
    "            probability.\n",
    "\n",
    "            This gives us a more accurate measurement of the Mean Absolute Error of the Trainee. If we only use the \n",
    "            actual prediction in our previous example, it might give us a skewed estimate, especially towards smaller\n",
    "            sample sizes. If our entire test set was 2 samples with identical probabilities like the example and both\n",
    "            of them were predicted wrong (predicted 1, actual 0), then using the raw prediction would give us a MAE of \n",
    "            2 while using `categorical_action_probabilities` will give us 1.1. For the opposite case it would give the \n",
    "            Trainee too much credit.\n",
    "\n",
    "            Since the prediction was essentialy a coin flip, using `categorical_action_probabilities` will ensure that \n",
    "            the accuracy reflects that model uncertainity. \n",
    "            \"\"\"\n",
    "            enumerated_explanation = enumerate(results['details']['categorical_action_probabilities'])\n",
    "            \n",
    "            # Create/reset the counter\n",
    "            total = 0\n",
    "            for i, prob in enumerated_explanation:\n",
    "                actual = y_test.iloc[i]\n",
    "                predicted = 1 - prob[feature][str(actual)] if str(actual) in prob[feature] else 1\n",
    "                total += predicted\n",
    "                accuracy =  total / len(y_test)\n",
    "\n",
    "        else:\n",
    "            accuracy = mean_absolute_error(y_test, results['action'][feature])\n",
    "        \n",
    "        # Package results into a DataFrame\n",
    "        result_df = all_df_results[feature]\n",
    "        result_df.loc[len(result_df.index)] = [residual, accuracy]\n",
    "        all_df_results[feature] = result_df\n",
    "    \n",
    "    trainee.release_resources()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9074e73d",
   "metadata": {},
   "source": [
    "# Step 3. Results\n",
    "\n",
    "We aggregate all the iterations by the mean of both metrics for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee40913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-09T15:53:39.820364Z",
     "iopub.status.busy": "2024-02-09T15:53:39.819955Z",
     "iopub.status.idle": "2024-02-09T15:53:39.837927Z",
     "shell.execute_reply": "2024-02-09T15:53:39.837505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>Global Feature Residuals</th>\n",
       "      <th>Prediction MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>target</td>\n",
       "      <td>0.255519</td>\n",
       "      <td>0.253611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>native-country</td>\n",
       "      <td>0.182360</td>\n",
       "      <td>0.182813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hours-per-week</td>\n",
       "      <td>8.546329</td>\n",
       "      <td>8.835556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>capital-loss</td>\n",
       "      <td>180.952324</td>\n",
       "      <td>192.708889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capital-gain</td>\n",
       "      <td>1783.030648</td>\n",
       "      <td>2009.387778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.275153</td>\n",
       "      <td>0.256794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>race</td>\n",
       "      <td>0.266943</td>\n",
       "      <td>0.247407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relationship</td>\n",
       "      <td>0.382891</td>\n",
       "      <td>0.371640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>occupation</td>\n",
       "      <td>0.815372</td>\n",
       "      <td>0.817473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>marital-status</td>\n",
       "      <td>0.355407</td>\n",
       "      <td>0.337899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>education-num</td>\n",
       "      <td>1.382684</td>\n",
       "      <td>1.381111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>education</td>\n",
       "      <td>0.537442</td>\n",
       "      <td>0.559695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>82857.780930</td>\n",
       "      <td>85407.512222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>workclass</td>\n",
       "      <td>0.489969</td>\n",
       "      <td>0.487798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>age</td>\n",
       "      <td>9.773260</td>\n",
       "      <td>9.648889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature  Global Feature Residuals  Prediction MAE\n",
       "0           target                  0.255519        0.253611\n",
       "1   native-country                  0.182360        0.182813\n",
       "2   hours-per-week                  8.546329        8.835556\n",
       "3     capital-loss                180.952324      192.708889\n",
       "4     capital-gain               1783.030648     2009.387778\n",
       "5              sex                  0.275153        0.256794\n",
       "6             race                  0.266943        0.247407\n",
       "7     relationship                  0.382891        0.371640\n",
       "8       occupation                  0.815372        0.817473\n",
       "9   marital-status                  0.355407        0.337899\n",
       "10   education-num                  1.382684        1.381111\n",
       "11       education                  0.537442        0.559695\n",
       "12          fnlwgt              82857.780930    85407.512222\n",
       "13       workclass                  0.489969        0.487798\n",
       "14             age                  9.773260        9.648889"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = pd.DataFrame()\n",
    "for feature in list(df):\n",
    "    final_output = pd.concat(\n",
    "            [pd.DataFrame([[feature] + list(all_df_results[feature].mean().values)], columns=['feature']+result_columns), final_output], \n",
    "            ignore_index=True\n",
    "    ) \n",
    "\n",
    "final_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d90abca",
   "metadata": {},
   "source": [
    "## 3a. Results\n",
    "\n",
    "We can see that the Global Feature Residuals and the Prediction MAE are extremely close for all features. In this experiment, the Prediction MAE represents the MAE from using a train-test split. These metrics converge even more as the number of runs increase.\n",
    "\n",
    "Using just the Global Feature Residuals has several efficiency benefits for the user. In addition to saving the user time and effort, it also allows the user to use all of the available data in training the Trainee. The smaller the dataset, the more important it is, as holding out 20% can represent a significant chunk of a small dataset. \n",
    "\n",
    "Conceptually, Global Feature Residuals represents a better representation of the Trainee's true performance. In machine learning, testing each individual case against a Trainee/model is often considered the most accurate form of validation called Leave-One-Out. However, this is very time consuming so generally data is tested in chunks, hence the use of the train-test split. Since Howso Engine already performs this Leave-One-Out validation, it allows users to take advantage of this method withou having to do it manually. \n",
    "\n",
    "Instance-based learning allows Howso Engine to do this effectively. For model-based learning methods performing Leave-One-Out, the model will need to be retrained every single case. In Howso Engine, predicting a case for feature residuals is equivalent to removing the case as demonstrated in `4-audit_edit` and then predicting the case using the Trainee with that case removed without needed to re-train or re-analyze. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69212f0c",
   "metadata": {},
   "source": [
    "# 4. Conclusion and Next Steps.\n",
    "\n",
    "\n",
    "This recipe was meant to both convince the user that the Global Feature Residuals is an equivalent substitute for Mean Absolute Error, thus removing the need for train-test split, and to provide further context into the capabilities of instance-based learning. Many of Howso Engine's unique capabilities may seem conceptually different since most are generally taught model-based techniques. By obtaining a deeper understanding of instance based-learning and Howso Engine, we can further develop use cases and capabilities that have not been previously explored and push the boundaries of Howso Engine's capabilities.\n",
    "\n",
    "Thank you for using Howso Engine we hope these Recipes have been useful to you. We here at Howso are beyond excited at the possibilities that you will come up with! The next step is to start exploring!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4993e365",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
